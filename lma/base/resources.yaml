apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: prometheus-operator
  labels:
    name: prometheus-operator
spec:
  releaseName: prometheus-operator
  wait: true # wait until all Pods, PVCs, Services
  targetNamespace: lma
  chart:
    git: https://github.com/helm/charts.git
    ref: master
    path: stable/prometheus-operator
  values:
    defaultRules:
      create: false
    alertmanager:
      enabled: false
    coreDns:
      enabled: false
    grafana:
      enabled: false
    kubeApiServer:
      enabled: false
    kubeControllerManager:
      enabled: false
    kubeDns:
      enabled: false
    kubelet:
      enabled: false
    kubeProxy:
      enabled: false
    kubeScheduler:
      enabled: false
    kubeStateMetrics:
      enabled: false
    nodeExporter:
      enabled: false
    prometheusOperator:
      enabled: true
      createCustomResource: true
      cleanupCustomResource: true
      cleanupCustomResourceBeforeInstall: true
    prometheus:
      enabled: false
--- 
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: elasticsearch-operator
  labels:
    name: elasticsearch-operator
spec:
  releaseName: elasticsearch-operator
  wait: true # wait until all Pods, PVCs, Services
  targetNamespace: elastic-system
  chart:
    git: https://github.com/openinfradev/taco-helm.git
    ref: master
    path: elasticsearch-operator
  values:
    elasticsearchOperator:
      enabled: true
      createNamespace: false
      createCustomResource: true
      #image:
      #  repository: registry.cicd.stg.taco/eck/eck-operator
      #  tag: 1.0.0
    customResource:
      elasticsearch:
        enabled: false
      kibana:
        enabled: false
    images:
      tags:
        eck-operator: registry.cicd.stg.taco/eck/eck-operator:1.0.0
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: prometheus
  labels:
    name: prometheus
spec:
  releaseName: prometheus
  wait: true # wait until all Pods, PVCs, Services
  targetNamespace: lma
  chart:
    git: https://github.com/helm/charts.git
    ref: master
    path: stable/prometheus-operator
  values:
    defaultRules:
      create: false
    alertmanager:
      enabled: false
    coreDns:
      enabled: true
    grafana:
      enabled: false
    kubeApiServer:
      enabled: true
    kubeControllerManager:
      enabled: true
    kubeDns:
      enabled: true
    kubeEtcd:
      enabled: true
    kubelet:
      enabled: true
    kubeProxy:
      enabled: true
    kubeScheduler:
      enabled: true
    kubeStateMetrics:
      enabled: false
    nodeExporter:
      enabled: false
    prometheusOperator:
      enabled: false
      admissionWebhooks:
        enabled: false
      createCustomResource: false
      cleanupCustomResource: false
      cleanupCustomResourceBeforeInstall: false
    prometheus:
      service:
        nodePort: 30008
        type: NodePort
      prometheusSpec:
        nodeSelector:
          taco-lma: enabled
        # image:
        #   repository: registry.cicd.stg.taco/prometheus/prometheus
        #   tag: v2.15.2
        serviceMonitorSelectorNilUsesHelmValues: false
        serviceMonitorNamespaceSelector:
          matchLabels:
            name: lma
        ruleSelectorNilUsesHelmValues: false
        ruleNamespaceSelector:
          matchLabels:
            name: lma
        externalLabels:
          taco_cluster: $(Values.site.spec.taco_cluster_name)
        secrets:
        - etcd-client-cert
        replicas: 1
        additionalScrapeConfigs:
        - job_name: kubernetes-service-endpoints
          scrape_interval: 1m
          scrape_timeout: 10s
          metrics_path: /metrics
          scheme: http
          kubernetes_sd_configs:
          - api_server: null
            role: endpoints
            namespaces:
              names: []
          relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            separator: ;
            regex: (.*node-exporter|openstack-metrics|prom-metrics|.*kube-state-metrics)
            replacement: $1
            action: drop
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            separator: ;
            regex: "true"
            replacement: $1
            action: keep
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
            separator: ;
            regex: (https?)
            target_label: __scheme__
            replacement: $1
            action: replace
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
            separator: ;
            regex: (.+)
            target_label: __metrics_path__
            replacement: $1
            action: replace
          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
            separator: ;
            regex: ([^:]+)(?::\d+)?;(\d+)
            target_label: __address__
            replacement: $1:$2
            action: replace
          - separator: ;
            regex: __meta_kubernetes_service_label_(.+)
            replacement: $1
            action: labelmap
          - source_labels: [__meta_kubernetes_namespace]
            separator: ;
            regex: (.*)
            target_label: kubernetes_namespace
            replacement: $1
            action: replace
          - source_labels: [__meta_kubernetes_service_name]
            separator: ;
            regex: (.*)
            target_label: kubernetes_name
            replacement: $1
            action: replace
          - source_labels: [__meta_kubernetes_service_name]
            separator: ;
            regex: (.*)
            target_label: job
            replacement: ${1}
            action: replace
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: rbd
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: $(Values.site.spec.volumes.prometheus)
    kubeEtcd:
      endpoints: $(Values.site.spec.k8sMasterIPs)
      serviceMonitor:
        scheme: https
        insecureSkipVerify: false
        serverName: localhost
        caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca
        certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client
        keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
    # images:
    #   tags:
    #     prometheus: registry.cicd.stg.taco/prometheus/prometheus:v2.15.2
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: prometheus-fed-master
  labels:
    name: prometheus-fed-master
spec:
  releaseName: prometheus-fed-master
  wait: true # wait until all Pods, PVCs, Services
  targetNamespace: fed
  chart:
    git: https://github.com/helm/charts.git
    ref: master
    path: stable/prometheus-operator
  values:
    fullnameOverride: fed-master
    defaultRules:
      create: true
    alertmanager:
      enabled: true
      alertmanagerSpec:
        nodeSelector:
          taco-lma: enabled
      config:
        global:
          smtp_smarthost: null
          smtp_from: null
          smtp_auth_username: null
          smtp_auth_password: null
          hipchat_auth_token: null
          hipchat_api_url: null
          slack_api_url: "https://hooks.slack.com/services/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"
        # templates:
        #   - '/etc/alertmanager/template/alert-templates.tmpl'
        route:
          group_by: ['alertname']
          group_wait: 10s
          repeat_interval: 1h
          receiver: 'default-alert'
          routes:
          - receiver: 'slack-alert'
            group_by: ['alertname']
            match:
              severity: page
        receivers:
        - name: 'default-alert'
          slack_configs:
          - channel: "#sample-noti"
            username: "Prometheus"
            send_resolved: true
            title: |-
              [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }} {{ range .Alerts.Firing }}{{ .Labels.alertname }}{{ end }}{{ range .Alerts.Resolved }}{{ .Labels.alertname }}{{ end }}{{ end }}
            text: |-
              {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }}
                {{ range .Alerts.Firing }}{{ .Annotations.message }}{{ end }}{{ range .Alerts.Resolved }}{{ .Annotations.message }}{{ end }}
              {{ else }}
              {{ if gt (len .Alerts.Firing) 0 }}
              *Alerts Firing:*
                {{ range .Alerts.Firing }}- {{ .Labels.alertname  }}: {{ .Annotations.message }}
              {{ end }}{{ end }}
              {{ if gt (len .Alerts.Resolved) 0 }}
              *Alerts Resolved:*
                {{ range .Alerts.Resolved }}- {{ .Labels.alertname }}: {{ .Annotations.message }}
              {{ end }}{{ end }}
              {{ end }}
        - name: 'slack-alert'
          slack_configs:
          - channel: "#sample-critical"
            username: "Prometheus"
            send_resolved: true
            title: |-
              [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }} {{ range .Alerts.Firing }}{{ .Labels.alertname }}{{ end }}{{ range .Alerts.Resolved }}{{ .Labels.alertname }}{{ end }}{{ end }}
            text: |-
              {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }}
                {{ range .Alerts.Firing }}{{ .Annotations.message }}{{ end }}{{ range .Alerts.Resolved }}{{ .Annotations.message }}{{ end }}
              {{ else }}
              {{ if gt (len .Alerts.Firing) 0 }}
              *Alerts Firing:*
                {{ range .Alerts.Firing }}- {{ .Labels.alertname }}: {{ .Annotations.message }}
              {{ end }}{{ end }}
              {{ if gt (len .Alerts.Resolved) 0 }}
              *Alerts Resolved:*
                {{ range .Alerts.Resolved }}- {{ .Labels.alertname }}: {{ .Annotations.message }}
              {{ end }}{{ end }}
              {{ end }}
        - name: 'telegram-alert'
          webhook_configs:
          - send_resolved: True
            url: http://prometheus-bot:9087/alert/-GROUP_ID
    coreDns:
      enabled: false
    grafana:
      enabled: false
      # because helm doesn't support installation for embeded chart, this manifest annotate blows

      # enabled: true
      # adminPassword: password
      # sidecar:
      #   dashboards:
      #     enabled: true
      #     label: grafana_dashboard
      #   datasources:
      #     enabled: true
      #     label: grafana_datasource
      # service:
      #   nodePort: 30009
      #   type: NodePort
      # persistence:
      #   enabled: true
      #   size: 10G
      #   storageClassName: rbd
      # # grafana.ini:
      # #   plugins:
      # #     vonage-status-panel: true
      # plugins:
      # - vonage-status-panel
      # - grafana-piechart-panel
      # image:
      #   tag: 6.5.2
    kubeApiServer:
      enabled: false
    kubeEtcd:
      enabled: false
    kubeControllerManager:
      enabled: false
    kubeDns:
      enabled: false
    kubelet:
      enabled: false
    kubeProxy:
      enabled: false
    kubeScheduler:
      enabled: false
    kubeStateMetrics:
      enabled: true
    nodeExporter:
      enabled: false
    prometheusOperator:
      enabled: false
      admissionWebhooks:
        enabled: false
      createCustomResource: false
      cleanupCustomResource: false
      cleanupCustomResourceBeforeInstall: false
    prometheus:
      service:
        nodePort: 30018
        type: NodePort
      prometheusSpec:
        nodeSelector:
          taco-lma: enabled
        serviceMonitorSelectorNilUsesHelmValues: false
        serviceMonitorNamespaceSelector:
          matchLabels:
            name: fed
        ruleSelectorNilUsesHelmValues: false
        ruleNamespaceSelector:
          matchLabels:
            name: fed
        # image:
        #   repository: registry.cicd.stg.taco/prometheus/prometheus
        #   tag: v2.15.2
        externalLabels:
          taco_cluster: federation-master
        replicas: 1
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: rbd
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: $(Values.site.spec.volumes.prometheus-fed-master)
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: kube-state-metrics
  labels:
    name: kube-state-metrics
spec:
  releaseName: kube-state-metrics
  wait: true # wait until all Pods, PVCs, Services
  targetNamespace: lma
  chart:
    git: https://github.com/helm/charts.git
    ref: master
    path: stable/kube-state-metrics
  #values:
  #   image:
  #     repository: registry.cicd.stg.taco/coreos/kube-state-metrics
  #     tag: v1.9.4
  #   images:
  #     tags:
  #       kube-state-metrics: registry.cicd.stg.taco/coreos/kube-state-metrics:v1.9.4
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: prometheus-node-exporter
  labels:
    name: prometheus-node-exporter
spec:
  releaseName: prometheus-node-exporter
  wait: true # wait until all Pods, PVCs, Services
  targetNamespace: lma
  chart:
    git: https://github.com/helm/charts.git
    ref: master
    path: stable/prometheus-node-exporter
  # values:
  #   image:
  #     repository: registry.cicd.stg.taco/prometheus/node-exporter
  #     tag: v0.18.1
  #   images:
  #     tags:
  #       node-exporter: registry.cicd.stg.taco/prometheus/node-exporter:v0.18.1
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: prometheus-pushgateway
  labels:
    name: prometheus-pushgateway
spec:
  releaseName: prometheus-pushgateway
  wait: true # wait until all Pods, PVCs, Services
  targetNamespace: lma
  chart:
    git: https://github.com/helm/charts.git
    ref: master
    path: stable/prometheus-pushgateway
  # values:
  #   image:
  #     repository: registry.cicd.stg.taco/coreos/prometheus-pushgateway
  #     tag: v1.9.4
  #   images:
  #     tags:
  #       prometheus-pushgateway: registry.cicd.stg.taco/coreos/prometheus-pushgateway:v1.9.4
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: prometheus-process-exporter
  labels:
    name: prometheus-process-exporter
spec:
  releaseName: prometheus-process-exporter
  wait: true # wait until all Pods, PVCs, Services
  targetNamespace: lma
  chart:
    git: https://github.com/openinfradev/openstack-helm-infra
    ref: temp
    path: prometheus-process-exporter
    skipDepUpdate: true
  values:
    # images:
    #   tags:
    #     process_exporter: registry.cicd.stg.taco/process-exporter:0.2.11
    #     dep_check: registry.cicd.stg.taco/kubernetes-entrypoint:v0.3.1
    #     image_repo_sync: registry.cicd.stg.taco/docker:17.07.0
    #   pull_policy: Always
    labels:
      process_exporter:
        process_selector_key: process-exporter
        process_selector_value: enabled
    pod:
      mandatory_access_control:
        type: null
      tolerations:
        process_exporter:
          enabled: true
          tolerations:
          - key: node-role.kubernetes.io/master
            operator: Exists
          - key: node-role.kubernetes.io/node
            operator: Exists
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: elasticsearch-kibana
  labels:
    name: elasticsearch-kibana
spec:
  releaseName: elasticsearch-kibana
  wait: true # wait until all Pods, PVCs, Services
  targetNamespace: lma
  chart:
    git: https://github.com/openinfradev/taco-helm
    ref: master
    path: elasticsearch-operator
  values:
    elasticsearchOperator:
      enabled: false
      # image:
      #   repository: registry.cicd.stg.taco/eck/eck-operator
      #   tag: 1.0.0
    customResource:
      elasticsearch:
        # image: registry.cicd.stg.taco/elasticsearch/elasticsearch
        version: 7.5.1
        enabled: true
        count: 3        # FIXME
        config:
          node.master: true
        volumeClaimTemplates:
        - metadata:
            name: elasticsearch-data
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: $(Values.site.spec.volumes.elasticsearch)
            storageClassName: $(Values.site.spec.storageClassName)
        nodeSelector:
          taco-lma: enabled
      kibana:
        # image: registry.cicd.stg.taco/kibana/kibana
        version: 7.4.2
        enabled: true
        http:
          tls:
            selfSignedCertificate:
              disabled: true
          service:
            spec:
              type: NodePort
              ports:
              - name: http
                nodePort: 30001
                targetPort: 5601
                port: 5601
        nodeSelector:
          taco-lma: enabled
    # images:
    #   tags:
    #     eck-operator: registry.cicd.stg.taco/eck/eck-operator:1.0.0
    #     elasticsearch: registry.cicd.stg.taco/elasticsearch/elasticsearch:7.5.1
    #     kibana: registry.cicd.stg.taco/kibana/kibana:7.4.2
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: grafana
  labels:
    name: grafana
spec:
  releaseName: grafana
  wait: true # wait until all Pods, PVCs, Services
  targetNamespace: fed
  chart:
    git: https://github.com/helm/charts.git
    ref: master
    path: stable/grafana
  values:
    adminPassword: password
    # image:
    #   repository: registry.cicd.stg.taco/grafana/grafana
    #   tag: 6.5.2
    # testFramework:
    #   image: registry.cicd.stg.taco/dduportal/bats
    #   tag: 0.4.0
    # initChownData:
    #   image:
    #     repository: busybox
    #     tag: 1.30
    sidecar:
      # image: registry.cicd.stg.taco/kiwigrid/k8s-sidecar:0.0.16
      dashboards:
        enabled: true
        label: grafana_dashboard
      datasources:
        enabled: true
        label: grafana_datasource
    service:
      type: NodePort
      nodePort: 30009
    persistence:
      enabled: true
      size: 10G
      storageClassName: rbd
    grafana.ini:
      plugins:
        vonage-status-panel: true
    plugins:
    - vonage-status-panel
    - grafana-piechart-panel

    # images:
    #   tags:
    #     grafana: registry.cicd.stg.taco/grafana/grafana:6.5.2
    #     testFramework: registry.cicd.stg.taco/dduportal/bats:0.4.0
    #     busybox: registry.cicd.stg.taco/busybox:1.30
    #     sidecar: registry.cicd.stg.taco/kiwigrid/k8s-sidecar:0.0.16
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: fluentbit
  labels:
    name: fluentbit
spec:
  releaseName: fluentbit
  wait: true # wait until all Pods, PVCs, Services
  targetNamespace: lma
  chart:
    git: https://github.com/openinfradev/openstack-helm-infra
    ref: temp
    path: fluentbit
    skipDepUpdate: true
  values:
    # images:
    #   tags:
    #     fluentbit: registry.cicd.stg.taco/fluent/fluent-bit:0.14.2
    #     dep_check: registry.cicd.stg.taco/airshipit/kubernetes-entrypoint:v1.0.0
    #     image_repo_sync: registry.cicd.stg.taco/docker:17.07.0
    #   pull_policy: IfNotPresent
    dependencies:
      static:
        fluentbit:
          jobs:
          services:
        tests:
          services:
    labels:
      fluentbit:
        node_selector_key: fluent-logging
        node_selector_value: enabled
    conf:
      fluentbit:
        template: |
          [SERVICE]
              Daemon false
              Flush 30
              Log_Level info
              Parsers_File parsers.conf

          [INPUT]
              Buffer_Chunk_Size 1M
              Buffer_Max_Size 1M
              Mem_Buf_Limit 5MB
              Name tail
              Parser docker
              Path /var/log/containers/*.log
              Tag kube.*

          [FILTER]
              Interval 1s
              Match **
              Name throttle
              Rate 1000
              Window 300

          [FILTER]
              Match kube.*
              Merge_JSON_Log true
              Name kubernetes

          [INPUT]
              Buffer_Chunk_Size 1M
              Buffer_Max_Size 1M
              Mem_Buf_Limit 5MB
              Name tail
              Parser syslog-kubelet
              Path /var/log/messages
              Tag syslog.*

          [INPUT]
              Buffer_Chunk_Size 1M
              Buffer_Max_Size 1M
              Mem_Buf_Limit 5MB
              Name tail
              Parser syslog-kubelet
              Path /var/log/syslog
              Tag syslog.*

          [FILTER]
              Match *
              Name record_modifier
              record cluster multinode-general

          [OUTPUT]
              Name  es
              Logstash_Format true
              Match kube.*
              Type fluent
              Host taco-elasticsearch-es-http
              Port 9200
              HTTP_User elastic
              HTTP_Passwd tacoword
              tls        On
              tls.verify Off

          [OUTPUT]
              Name  es
              Logstash_Format true
              Logstash_Prefix syslog
              Match syslog.*
              Type syslog
              Host taco-elasticsearch-es-http
              Port 9200
              HTTP_User elastic
              HTTP_Passwd tacoword
              tls        On
              tls.verify Off

      parsers:
        template: |
          [PARSER]
            Decode_Field_As escaped_utf8 log
            Format json
            Name docker
            Time_Format %Y-%m-%dT%H:%M:%S.%L
            Time_Keep true
            Time_Key time
          [PARSER]
            NAME syslog-kubelet
            Format regex
            Regex '^(?<time>.*[0-9]{2}:[0-9]{2}:[0-9]{2}) (?<host>[^ ]*) (?<app>[a-zA-Z0-9_\/\.\-]*)(?:\[(?<pid>[0-9]+)\])?(?:[^\:]*\:)? (?<log>.+)$'
            Time_Key time
            Time_Format '%b %e %H:%M:%S'
            Time_Keep On
            Decode_Field_As escaped_utf8 log
    pod:
      tolerations:
        fluentbit:
          enabled: true
          tolerations:
          - key: node-role.kubernetes.io/master
            operator: Exists
          - key: node-role.kubernetes.io/node
            operator: Exists
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: addons
  labels:
    name: addons
spec:
  releaseName: addons
  wait: true # wait until all Pods, PVCs, Services
  targetNamespace: lma
  chart:
    git: https://github.com/openinfradev/taco-addons.git
    ref: master
    path: lma
  values:
    metricbeat:
      # image: registry.cicd.stg.taco/beats/metricbeat:taco-1.0.0
      enabled: $(Values.site.spec.metricbeat.enabled)
      elasticsearch:
        host: "https://taco-elasticsearch-es-http:9200"
        username: elastic
        password: tacoword
      kibana:
        host: "taco-kibana-dashboard-kb-http:5601"
      prometheus:
        hosts: ["fed-master-prometheus.fed.svc.cluster.local:9090"]
      addtionalModules: []
    grafanaDashboard:
      enabled: false
    serviceMonitor:
      processExporter:
        enabled: true
        selector:
          matchLabels:
            application: process_exporter
            component: metrics
            release_group: prometheus-process-exporter
      grafana:
        enabled: false
      openstack:
        enabled: $(Values.site.spec.serviceMonitor.openstack)
      ceph:
        enabled: $(Values.site.spec.serviceMonitor.ceph)
        mon_hosts: $(Values.site.spec.ceph_mon_hosts)
    tacoWatcher:
      rbac:
        create: false
    prometheusRules:
      aggregation:
        enabled: true
    # images:
    #   tags:
    #     metricbeat: registry.cicd.stg.taco/beats/metricbeat:taco-1.0.0
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: fed-addons
  labels:
    name: fed-addons
spec:
  releaseName: fed-addons
  wait: true # wait until all Pods, PVCs, Services
  targetNamespace: fed
  chart:
    git: https://github.com/openinfradev/taco-addons.git
    ref: master
    path: lma
  values:
    metricbeat:
      enabled: false
    grafanaDashboard:
      enabled: true
      sidecar:
        datasources:
          prometheusAddress: "fed-master-prometheus:9090"
    serviceMonitor:
      processExporter:
        enabled: false
      grafana:
        enabled: true
      ceph:
        enabled: false
      kubeStateMetrics:
        enabled: false
      nodeExporter:
        enabled: false
      additionalScrapeConfigs:
      - name: main-federate
        interval: 45s
        path: /federate
        addresses: $(Values.site.spec.fed_prometheus_addresses)
        port: 30008
        params:
          match[]:
          - '{job=~".*exporter|.*kube-state-metrics|ceph.*|openstack.*"}'
          - '{__name__=~"etcd_server_has_leader|node_cpu_seconds_total|node_memory_MemTotal_bytes|node_memory_MemFree_bytes|node_memory_MemFree_bytes|node_network_receive_bytes_total|node_network_trans mit_bytes_total|namedprocess_namegroup_cpu_user_seconds_total|coredns_build_info|up|ceph_health_status" }'
          -  '{__name__=~"container_cpu_user_seconds_total|container_cpu_usage_seconds_total|kube_pod_container_reso urce_requests_cpu_cores|kube_pod_container_resource_limits_cpu_cores|container_memory_working_set_bytes |kube_pod_container_resource_requests_memory_bytes|kube_pod_container_resource_limits_memory_bytes|cont ainer_network_transmit_bytes_total|container_network_receive_bytes_total"}'
    tacoWatcher:
      host: $(Values.site.spec.externalIP)
      port: 32000
      rbac:
        create: true
      joinCluster:
        enabled: true
        body:
          isMain: true
          kibanaUrl: "http://taco-kibana-dashboard-kb-http.lma.svc.cluster.local:5601"
          grafanaUrl: "http://grafana.fed.svc.cluster.local"
          k8sUrl: "https://kubernetes.default.svc.cluster.local"
          menu:
            enabled: true
    kibanaInit:
      enabled: true
      url: http://$(Values.site.spec.externalIP):30001
      image: sktdev/kibana-init:v4
    prometheusRules:
      alert:
        enabled: true
      create: true
      rule:
        alertmanager: true
        basicLinux: true
        calico: true
        ceph: true
        elasticsearch: true
        kubeApiserver: true
        kubeControllerManager: true
        kubelet: true
        mariadb: true
        rabbitmq: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: taco-watcher
  labels:
    name: taco-watcher
spec:
  releaseName: taco-watcher
  wait: true # wait until all Pods, PVCs, Services
  targetNamespace: fed
  chart:
    git: https://github.com/openinfradev/taco-helm
    ref: master
    path: taco-watcher
  values:
    nodeSelector:
      taco-lma: enabled
    image:
      repository: sktdev/taco-watcher
      tag: 1.0.4
    service:
      type: NodePort
      port: 32000
      targetPort: 32000
      nodePort: 32000
      proxy_from: 32001
      proxy_to: 32009
    resources:
      storageClassName: rbd
    config:
      initDB: true
      username: taco
      password: password
      kibana:
        authkey: elastic:tacoword
      grafana:
        authkey: admin:password
